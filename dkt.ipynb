{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'\n",
    "#train_file = os.path.join(DATA_DIR, 'builder_train.csv')\n",
    "#test_file = os.path.join(DATA_DIR, 'builder_test.csv')\n",
    "train_file = os.path.join(DATA_DIR, '0910_b_train.csv')\n",
    "test_file = os.path.join(DATA_DIR, '0910_b_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data_from_csv(filename):\n",
    "    rows = []\n",
    "    max_num_problems_answered = 0\n",
    "    num_problems = 0\n",
    "    \n",
    "    print(\"Reading {0}\".format(filename))\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "    print(\"{0} lines was read\".format(len(rows)))\n",
    "    \n",
    "    # tuples stores the student answering sequence as \n",
    "    # ([num_problems_answered], [problem_ids], [is_corrects])\n",
    "    tuples = []\n",
    "    for i in range(0, len(rows), 3):\n",
    "        # numbers of problem a student answered\n",
    "        num_problems_answered = int(rows[i][0])\n",
    "        \n",
    "        # only keep student with at least 3 records.\n",
    "        if num_problems_answered < 3:\n",
    "            continue\n",
    "        \n",
    "        problem_ids = rows[i+1]\n",
    "        is_corrects = rows[i+2]\n",
    "        \n",
    "        invalid_ids_loc = [i for i, pid in enumerate(problem_ids) if pid=='']        \n",
    "        for invalid_loc in invalid_ids_loc:\n",
    "            del problem_ids[invalid_loc]\n",
    "            del is_corrects[invalid_loc]\n",
    "        \n",
    "        tup =(num_problems_answered, problem_ids, is_corrects)\n",
    "        tuples.append(tup)\n",
    "        \n",
    "        if max_num_problems_answered < num_problems_answered:\n",
    "            max_num_problems_answered = num_problems_answered\n",
    "        \n",
    "        pid = max(int(pid) for pid in problem_ids if pid!='')\n",
    "        if num_problems < pid:\n",
    "            num_problems = pid\n",
    "    # add 1 to num_problems because 0 is in the pid\n",
    "    num_problems+=1\n",
    "\n",
    "    #shuffle the tuple\n",
    "    random.shuffle(tuples)\n",
    "\n",
    "    print (\"max_num_problems_answered:\", max_num_problems_answered)\n",
    "    print (\"num_problems:\", num_problems)\n",
    "    print(\"The number of students is {0}\".format(len(tuples)))\n",
    "    print(\"Finish reading data.\")\n",
    "    \n",
    "    return tuples, max_num_problems_answered, num_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding(question_seq, question_corr, target_length):\n",
    "    num_questions = len(question_seq)\n",
    "    pad_length = target_length - num_questions\n",
    "    question_seq += [-1]*pad_length\n",
    "    question_corr += [0]*pad_length\n",
    "    return (question_seq, question_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./data/0910_b_train.csv\n",
      "10116 lines was read\n",
      "max_num_problems_answered: 1219\n",
      "num_problems: 124\n",
      "The number of students is 3134\n",
      "Finish reading data.\n",
      "time used: 0.29899144172668457s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "students_train, max_num_problems_answered_train, num_problems = \\\n",
    "read_data_from_csv(train_file)\n",
    "\n",
    "students_train = [padding(q_seq, q_corr, max_num_problems_answered_train) \n",
    "                  for _, q_seq, q_corr in students_train]\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"time used: {0}s\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model\n",
    "\n",
    "### Placeholder Explanation\n",
    "X is the one-hot encoded input sequence of a student.\n",
    "y is the one-hot encoded correct sequence of a student.\n",
    "\n",
    "For example, the student i has a seq [1, 3, 1, 1, 2] with correct map [0, 1, 1, 1, 0]. The X_seq will be one hot encoded as:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The X_corr map will be one hot encoded as:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&0&0&0&0\\\\\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Then, it will be concatenated into $X^i$:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc|ccccc}\n",
    "        0&1&0&0&0&0&0&0&0&0\\\\\n",
    "        0&0&0&1&0&0&0&0&1&0\\\\\n",
    "        0&1&0&0&0&0&1&0&0&0\\\\\n",
    "        0&1&0&0&0&0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The last question '2' is not used in the $X^i$ because it is the last record that the student has and therefore used in $y$.\n",
    "So, $y$ would be seq [3, 1, 1, 2] with corr map [1, 1, 1, 0]\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_corr_to_onehot(seq, corr, num_steps, num_problems):\n",
    "    seq_oh = tf.one_hot(seq, depth=num_problems)\n",
    "    seq_oh_flat = tf.reshape(seq_oh, [-1, num_problems])\n",
    "    \n",
    "    # element-wise multiplication between Matrix and Vector\n",
    "    # the i-th column of Matrixelement-wisedly multiply the i-th element in the Vector\n",
    "    corr_flat = tf.reshape(corr, [-1])\n",
    "    corr_mat = tf.multiply(tf.transpose(seq_oh_flat), tf.cast(corr_flat, dtype=tf.float32))\n",
    "    corr_mat = tf.transpose(corr_mat)\n",
    "    corr_mat = tf.reshape(corr_mat, shape=[-1, num_steps, num_problems])\n",
    "    \n",
    "    return seq_oh, corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# network configuration\n",
    "batch_size = 32\n",
    "num_layers = 1\n",
    "state_size = 200\n",
    "num_steps = max_num_problems_answered_train-1\n",
    "input_size = num_problems * 2\n",
    "output_size = num_problems\n",
    "\n",
    "inputs_seq = tf.placeholder(tf.int32, [None, num_steps])\n",
    "inputs_corr = tf.placeholder(tf.int32, [None, num_steps])\n",
    "X_seq, X_corr = seq_corr_to_onehot(inputs_seq, inputs_corr, num_steps, num_problems)\n",
    "X = tf.concat([X_seq, X_corr], axis=2, name='X')\n",
    "X = tf.cast(X, dtype=tf.float32)\n",
    "\n",
    "targets_seq = tf.placeholder(tf.int32, [None, num_steps])\n",
    "targets_corr = tf.placeholder(tf.int32, [None, num_steps])\n",
    "y_seq, y_corr = seq_corr_to_onehot(targets_seq, targets_corr, num_steps, num_problems)\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, None, state_size])\n",
    "state_per_layer_list  = tf.unstack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple([tf.contrib.rnn.LSTMStateTuple(\n",
    "            state_per_layer_list[idx][0],\n",
    "            state_per_layer_list[idx][1]\n",
    "        ) for idx in range(num_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'strided_slice:0' shape=(?, 200) dtype=float32>, h=<tf.Tensor 'strided_slice_1:0' shape=(?, 200) dtype=float32>),)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_tuple_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Configuration\n",
    "There are basically 2 elements needed to construct the LSTM network\n",
    "1. The cell, and\n",
    "2. The rnn structure.\n",
    "\n",
    "The cell is defined via the tf.contrib.rnn library. It supports the multilayer RNN as well. \n",
    "\n",
    "The RNN is defined via the tf.nn.dynamic_rnn. It is parameterized by the cell defined, the input X, and a initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the states series is:\n",
      " Tensor(\"rnn/rnn/transpose:0\", shape=(?, 1218, 200), dtype=float32)\n",
      "\n",
      "the current_state is:\n",
      " (LSTMStateTuple(c=<tf.Tensor 'rnn/rnn/while/Exit_2:0' shape=(?, 200) dtype=float32>, h=<tf.Tensor 'rnn/rnn/while/Exit_3:0' shape=(?, 200) dtype=float32>),)\n"
     ]
    }
   ],
   "source": [
    "# build up the network\n",
    "with tf.variable_scope('cell'):\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units=state_size,\n",
    "                                   forget_bias=1.0,\n",
    "                                   state_is_tuple=True)\n",
    "    \n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                        output_keep_prob=1.0)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell]*num_layers, state_is_tuple=True)\n",
    "\n",
    "with tf.variable_scope('rnn'):\n",
    "    states_series, current_state = tf.nn.dynamic_rnn(cell, \n",
    "                                                    X,\n",
    "                                                    initial_state=rnn_tuple_state,\n",
    "                                                    time_major=False)\n",
    "\n",
    "print(\"the states series is:\\n\", states_series)\n",
    "print(\"\\nthe current_state is:\\n\", current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_yh = tf.Variable(tf.random_normal([state_size, output_size]), name=\"W_yh\")\n",
    "b_yh = tf.Variable(tf.constant(0.1, shape=[output_size,]), name=\"b_yh\")\n",
    "\n",
    "states_series = tf.reshape(states_series, [-1, state_size])\n",
    "logits_flat = tf.matmul(states_series, W_yh) + b_yh\n",
    "y_seq_flat = tf.cast(tf.reshape(y_seq, [-1, output_size]), dtype=tf.float32)\n",
    "y_corr_flat = tf.cast(tf.reshape(y_corr, [-1, output_size]), dtype=tf.float32)\n",
    "\n",
    "target_logits = tf.multiply(logits_flat, y_seq_flat)\n",
    "target_logits = tf.reduce_sum(target_logits, axis=1)\n",
    "\n",
    "target_labels = tf.multiply(y_corr_flat, y_seq_flat)\n",
    "target_labels = tf.reduce_sum(target_labels, axis=1)\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=target_logits, \n",
    "                                               labels=target_labels)\n",
    "total_loss = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_train(sess):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        num_students = len(students_train)\n",
    "        \n",
    "        iteration = 0\n",
    "        for batch_idx in range(0, num_students, batch_size):\n",
    "            start_idx = batch_idx\n",
    "            end_idx = min(num_students, batch_idx+batch_size)\n",
    "            \n",
    "            new_batch_size = end_idx - start_idx\n",
    "            _current_state = np.zeros((num_layers, 2, new_batch_size, state_size))\n",
    "            \n",
    "            inputs_seq_batch = np.array([tup[0][:-1] for tup in students_train[start_idx:end_idx]], dtype=np.int32)\n",
    "            inputs_corr_batch = np.array([tup[1][:-1] for tup in students_train[start_idx:end_idx]], dtype=np.int32)\n",
    "            \n",
    "            y_seq_batch = np.array([tup[0][1:] for tup in students_train[start_idx:end_idx]], dtype=np.int32)\n",
    "            y_corr_batch = np.array([tup[1][1:] for tup in students_train[start_idx:end_idx]], dtype=np.int32)\n",
    "            \n",
    "#             print(inputs_seq_batch.shape, \n",
    "#                  inputs_corr_batch.shape,\n",
    "#                  y_seq_batch.shape,\n",
    "#                  y_corr_batch.shape)\n",
    "\n",
    "            _optimizer, _current_state = sess.run([optimizer, current_state],\n",
    "                    feed_dict={\n",
    "                    inputs_seq: inputs_seq_batch,\n",
    "                    inputs_corr: inputs_corr_batch,\n",
    "                    targets_seq: y_seq_batch,\n",
    "                    targets_corr: y_corr_batch,\n",
    "                    init_state: _current_state,\n",
    "                })\n",
    "            \n",
    "            if iteration%100 == 0:\n",
    "                _total_loss= sess.run([total_loss],\n",
    "                    feed_dict={\n",
    "                    inputs_seq: inputs_seq_batch,\n",
    "                    inputs_corr: inputs_corr_batch,\n",
    "                    targets_seq: y_seq_batch,\n",
    "                    targets_corr: y_corr_batch,\n",
    "                    init_state: _current_state,\n",
    "                })\n",
    "                print(\"Epoch {0}, batch {1}, loss value: {2}\".format(epoch_idx, batch_idx, _total_loss))\n",
    "            \n",
    "            iteration+=1\n",
    "\n",
    "                \n",
    "                \n",
    "def run_test(sess):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0, loss value: [0.68823177]\n",
      "Epoch 1, batch 0, loss value: [0.68385661]\n",
      "Epoch 2, batch 0, loss value: [0.68093604]\n",
      "Epoch 3, batch 0, loss value: [0.6787768]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9a5bf1ec2a0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mrun_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-52474a8fb914>\u001b[0m in \u001b[0;36mrun_train\u001b[1;34m(sess)\u001b[0m\n\u001b[0;32m     31\u001b[0m                     \u001b[0mtargets_seq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_seq_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[0mtargets_corr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_corr_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                     \u001b[0minit_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_current_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                 })\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\YEUNG\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\YEUNG\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\YEUNG\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\YEUNG\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\YEUNG\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "WITH_CONFIG = True\n",
    "num_epochs = 25\n",
    "\n",
    "start_time = time.time()\n",
    "if WITH_CONFIG:\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    with tf.Session(config=config) as sess:\n",
    "        run_train(sess)\n",
    "else:\n",
    "    with tf.Session() as sess:\n",
    "        run_train(sess)\n",
    "           \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"program run for: {0}s\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.randint(124, size=(3, 10))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session():\n",
    "    x = tf.cast([[0,1,0,0,1]], dtype=tf.int32)\n",
    "    zero = tf.constant(0, dtype=tf.int32)\n",
    "    where = tf.not_equal(x, zero)\n",
    "    indices = tf.where(where)\n",
    "    print(tf.gather(x, indices).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
