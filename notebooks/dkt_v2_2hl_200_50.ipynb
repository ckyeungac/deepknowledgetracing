{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import load_train_test\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# specify the gpu device\n",
    "# import os\n",
    "# from Tools.utils import _make_dir, load_options\n",
    "# options = load_options('options.json')\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"OCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "SPLIT_MSG=\"***********\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_train_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9bb8f0e33d44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstudents_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudents_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_num_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_problems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_train_test' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data/'\n",
    "train_file = '0910_b_train.csv'\n",
    "test_file = '0910_b_test.csv'\n",
    "train_path= os.path.join(DATA_DIR, train_file)\n",
    "test_path = os.path.join(DATA_DIR, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "students_train, students_test, max_num_steps, num_problems = load_train_test(train_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model\n",
    "\n",
    "### Placeholder Explanation\n",
    "X is the one-hot encoded input sequence of a student.\n",
    "y is the one-hot encoded correct sequence of a student.\n",
    "\n",
    "For example, the student i has a seq [1, 3, 1, 2, 2] with correct map [0, 1, 1, 0, 0]. The X_seq will be one hot encoded as:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&1&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The X_corr map will be one hot encoded as:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&0&0&0&0\\\\\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Our desire $X^i$ will be encoded as the following:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&-1&0&0&0\\\\\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "The last question '2' is not used in the $X^i$ because it is the last record that the student has and therefore used in $y$.\n",
    "So, $y$ would be seq [3, 1, 2, 2] with corr map [1, 1, 0, 0]\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&0&0&0\\\\\n",
    "        0&0&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_corr_to_onehot(seq, corr, num_steps, num_problems):\n",
    "    seq_oh = tf.one_hot(seq, depth=num_problems)\n",
    "    seq_oh_flat = tf.reshape(seq_oh, [-1, num_problems])\n",
    "    \n",
    "    # element-wise multiplication between Matrix and Vector\n",
    "    # the i-th column of Matrixelement-wisedly multiply the i-th element in the Vector\n",
    "    corr_flat = tf.reshape(corr, [-1])\n",
    "    corr_mat = tf.multiply(tf.transpose(seq_oh_flat), tf.cast(corr_flat, dtype=tf.float32))\n",
    "    corr_mat = tf.transpose(corr_mat)\n",
    "    corr_mat = tf.reshape(corr_mat, shape=[-1, num_steps, num_problems])\n",
    "    \n",
    "    corr_mat_value_two = corr_mat * 2\n",
    "    \n",
    "    X = corr_mat_value_two - seq_oh\n",
    "    \n",
    "    return seq_oh, corr_mat, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    length = tf.reduce_sum(used, 1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# network configuration\n",
    "batch_size = 32\n",
    "max_num_steps = max_num_steps - 1\n",
    "num_problems = num_problems\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "inputs_seq = tf.placeholder(tf.int32, [None, max_num_steps])\n",
    "inputs_corr = tf.placeholder(tf.int32, [None, max_num_steps])\n",
    "X_seq, X_corr, X = seq_corr_to_onehot(inputs_seq, inputs_corr, max_num_steps, num_problems)\n",
    "\n",
    "targets_seq = tf.placeholder(tf.int32, [None, max_num_steps])\n",
    "targets_corr = tf.placeholder(tf.int32, [None, max_num_steps])\n",
    "y_seq, y_corr, _ = seq_corr_to_onehot(targets_seq, targets_corr, max_num_steps, num_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1218), Dimension(124)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hidden layer 2 states series is:\n",
      " Tensor(\"hidden_layer_2/rnn/transpose:0\", shape=(?, 1218, 50), dtype=float32)\n",
      "\n",
      "the current_state is:\n",
      " LSTMStateTuple(c=<tf.Tensor 'hidden_layer_2/rnn/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'hidden_layer_2/rnn/while/Exit_3:0' shape=(?, 50) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "# build up the network\n",
    "hl1_size = 200\n",
    "hl2_size = 50\n",
    "sequence_length = length(X_seq)\n",
    "\n",
    "with tf.variable_scope('hidden_layer_1'):\n",
    "    hl1_cell = tf.contrib.rnn.LSTMCell(num_units=hl1_size)\n",
    "    hl1_cell = tf.contrib.rnn.DropoutWrapper(hl1_cell, output_keep_prob=keep_prob)\n",
    "    hl1_output, hl1_state = tf.nn.dynamic_rnn(\n",
    "        hl1_cell,\n",
    "        X,\n",
    "        dtype=tf.float32,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "\n",
    "with tf.variable_scope('hidden_layer_2'):\n",
    "    hl2_cell = tf.contrib.rnn.LSTMCell(num_units=hl2_size)\n",
    "    hl2_cell = tf.contrib.rnn.DropoutWrapper(hl2_cell, output_keep_prob=keep_prob)\n",
    "    hl2_output, hl2_state = tf.nn.dynamic_rnn(\n",
    "        hl2_cell,\n",
    "        hl1_output,\n",
    "        dtype=tf.float32,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"the hidden layer 2 states series is:\\n\", hl2_output)\n",
    "print(\"\\nthe current_state is:\\n\", hl2_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(output, target):\n",
    "    # Compute cross entropy for each frame\n",
    "    cross_entropy = target * tf.log(output)\n",
    "    cross_entropy = -tf.reduce_sum(cross_entropy, 2)\n",
    "    mask = tf.sign(tf.reduce_max(tf.abs(target), 2))\n",
    "    cross_entropy *= mask\n",
    "    \n",
    "    # Average over actual sequence lengths\n",
    "    cross_entropy = tf.reduce_sum(cross_entropy, 1)\n",
    "    cross_entropy /= tf.reduce_sum(mask, 1)\n",
    "    \n",
    "    cost = tf.reduce_mane(cross_entropy)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this code block calculate the loss using tf.gather_nd\n",
    "W_yh = tf.Variable(tf.random_normal([hl2_size, num_problems]), name=\"W_yh\")\n",
    "b_yh = tf.Variable(tf.constant(0.1, shape=[num_problems,]), name=\"b_yh\")\n",
    "\n",
    "last_layer_output = tf.reshape(hl2_output, [-1, hl2_size])\n",
    "logits_flat = tf.matmul(last_layer_output, W_yh) + b_yh\n",
    "preds_flat = tf.sigmoid(logits_flat)\n",
    "y_seq_flat = tf.cast(tf.reshape(y_seq, [-1, num_problems]), dtype=tf.float32)\n",
    "y_corr_flat = tf.cast(tf.reshape(y_corr, [-1, num_problems]), dtype=tf.float32)\n",
    "\n",
    "# get the indices where they are not equal to 0\n",
    "# the indices implies that a student has answered the question in the time step\n",
    "# and thereby exclude those time step that the student hasn't answered.\n",
    "target_indices = tf.where(tf.not_equal(y_seq_flat, 0))\n",
    "target_logits = tf.gather_nd(logits_flat, target_indices)\n",
    "target_preds = tf.gather_nd(preds_flat, target_indices)\n",
    "target_labels = tf.gather_nd(y_corr_flat, target_indices)\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=target_logits, \n",
    "                                               labels=target_labels)\n",
    "total_loss = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize(sess, print_loss=False):    \n",
    "    students = students_train\n",
    "    \n",
    "    best_test_auc = 0\n",
    "    best_epoch_idx = 0\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        \n",
    "        num_students = 10\n",
    "        num_students = len(students) \n",
    "\n",
    "        loss_train = 0\n",
    "        iteration = 1\n",
    "        \n",
    "        for batch_idx in range(0, num_students, batch_size):\n",
    "            start_idx = batch_idx\n",
    "            end_idx = min(num_students, batch_idx+batch_size)\n",
    "            \n",
    "            new_batch_size = end_idx - start_idx\n",
    "            \n",
    "            inputs_seq_batch = np.array([tup[1][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "            inputs_corr_batch = np.array([tup[2][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "            \n",
    "            y_seq_batch = np.array([tup[1][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "            y_corr_batch = np.array([tup[2][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "\n",
    "            _optimizer, _target_preds, _target_labels, _total_loss = sess.run(\n",
    "                    [optimizer, target_preds, target_labels, total_loss],\n",
    "                    feed_dict={\n",
    "                    inputs_seq: inputs_seq_batch,\n",
    "                    inputs_corr: inputs_corr_batch,\n",
    "                    targets_seq: y_seq_batch,\n",
    "                    targets_corr: y_corr_batch,\n",
    "                    keep_prob: 0.5,\n",
    "                })\n",
    "            \n",
    "            y_pred += [p for p in _target_preds]\n",
    "            y_true += [t for t in _target_labels]\n",
    "            loss_train = (iteration-1)/(iteration) * loss_train + _total_loss/iteration\n",
    "            iteration+=1\n",
    "        \n",
    "        # Print training information        \n",
    "        fpr, tpr, thres = roc_curve(y_true, y_pred, pos_label=1)\n",
    "        auc_train = auc(fpr, tpr)\n",
    "        print('Epoch {0:>4}, Train AUC: {1:.5}, Train Loss: {2:.5}'.format(epoch_idx+1, auc_train, loss_train))\n",
    "        \n",
    "        # evaluate on the test set\n",
    "        auc_test, loss_test = evaluate(sess, is_train=False)\n",
    "        test_msg = \"Epoch {0:>4}, Test AUC: {1:.5}, Test Loss: {2:.5}\".format(epoch_idx+1, auc_test, loss_test)\n",
    "        if auc_test > best_test_auc:\n",
    "            test_msg += \"*\"\n",
    "            best_epoch_idx = epoch_idx\n",
    "            best_test_auc = auc_test\n",
    "            saver.save(sess=sess, save_path=save_path)\n",
    "        print(test_msg)\n",
    "        print(SPLIT_MSG)        \n",
    "        # quit the training if there is no improve in AUC for 20 epochs.\n",
    "        if epoch_idx - best_epoch_idx >= 20:\n",
    "            print(\"No improvement shown in 20 epochs. Quit Training.\")\n",
    "            break\n",
    "    \n",
    "    print(\"The best testing result occured at: {0}-th epoch, with testing AUC: {1:.5}\".format(best_epoch_idx, best_test_auc))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def evaluate(sess, is_train=False):    \n",
    "    if is_train:\n",
    "        students = students_train\n",
    "    else:\n",
    "        students = students_test\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    iteration = 1\n",
    "    _loss = 0\n",
    "    \n",
    "    num_students = 10\n",
    "    num_students = len(students)\n",
    "    \n",
    "    for batch_idx in range(0, num_students, batch_size):\n",
    "        start_idx = batch_idx\n",
    "        end_idx = min(num_students, batch_idx+batch_size)\n",
    "\n",
    "        new_batch_size = end_idx - start_idx\n",
    "\n",
    "        inputs_seq_batch = np.array([tup[1][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "        inputs_corr_batch = np.array([tup[2][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "\n",
    "        y_seq_batch = np.array([tup[1][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "        y_corr_batch = np.array([tup[2][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "\n",
    "        _target_preds, _target_labels, _total_loss = sess.run(\n",
    "                [target_preds, target_labels, total_loss],\n",
    "                feed_dict={\n",
    "                inputs_seq: inputs_seq_batch,\n",
    "                inputs_corr: inputs_corr_batch,\n",
    "                targets_seq: y_seq_batch,\n",
    "                targets_corr: y_corr_batch,\n",
    "                keep_prob: 1,\n",
    "            })\n",
    "\n",
    "        y_pred += [p for p in _target_preds]\n",
    "        y_true += [t for t in _target_labels]\n",
    "        _loss = (iteration-1)/(iteration) * _loss + _total_loss/iteration\n",
    "        iteration+=1\n",
    "\n",
    "    fpr, tpr, thres = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    return (auc_score, _loss)\n",
    "\n",
    "def get_student_output_layer(sess, student):\n",
    "    num_steps = len(student[1]) - 1\n",
    "    shape = (1, num_steps)\n",
    "    _inputs_seq = np.array(student[1][:-1]).reshape(shape)\n",
    "    _inputs_corr = np.array(student[2][:-1]).reshape(shape)\n",
    "    \n",
    "    _y_seq = np.array(student[1][1:]).reshape(shape)\n",
    "    _y_corr = np.array(student[2][1:]).reshape(shape)\n",
    "    \n",
    "    _preds_flat = sess.run(\n",
    "        preds_flat,\n",
    "        feed_dict={\n",
    "            inputs_seq: _inputs_seq,\n",
    "            inputs_corr: _inputs_corr,\n",
    "            targets_seq: _y_seq,\n",
    "            targets_corr: _y_corr,\n",
    "            keep_prob: 1,\n",
    "        }\n",
    "    )    \n",
    "    return _preds_flat\n",
    "\n",
    "def get_student_hidden_layer(sess, student, layer_num=1):\n",
    "    num_steps = len(student[1]) - 1\n",
    "    shape = (1, num_steps)\n",
    "    _inputs_seq = np.array(student[1][:-1]).reshape(shape)\n",
    "    _inputs_corr = np.array(student[2][:-1]).reshape(shape)\n",
    "    \n",
    "    _y_seq = np.array(student[1][1:]).reshape(shape)\n",
    "    _y_corr = np.array(student[2][1:]).reshape(shape)\n",
    "    \n",
    "    \n",
    "    feed_dict={\n",
    "            inputs_seq: _inputs_seq,\n",
    "            inputs_corr: _inputs_corr,\n",
    "            targets_seq: _y_seq,\n",
    "            targets_corr: _y_corr,\n",
    "            keep_prob: 1,\n",
    "        }\n",
    "    \n",
    "    result = None\n",
    "    if layer_num == 1:\n",
    "        result = sess.run(\n",
    "            hl1_output,\n",
    "            feed_dict=feed_dict,\n",
    "        )\n",
    "    elif layer_num == 2:\n",
    "        result = sess.run(\n",
    "            hl2_output,\n",
    "            feed_dict=feed_dict,\n",
    "        )\n",
    "    else:\n",
    "        print(\"layer is not available\")\n",
    "        return None\n",
    "    return result[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define the tf saver\n",
    "saver = tf.train.Saver()\n",
    "save_dir = 'checkpoints/2hl_200_50/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WITH_CONFIG = True\n",
    "num_epochs = 1000\n",
    "\n",
    "### Start Training\n",
    "start_time = time.time()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        saver.restore(sess=sess, save_path=save_path)\n",
    "        print(\"Pre-trained model found, loading the previous variables\")\n",
    "    except:\n",
    "        print(\"Pre-trained model not found, train from scratch now.\")\n",
    "    optimize(sess)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"program run for: {0}s\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the saved variable to the current session.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/2hl_200_50/model\n",
      "auc_test: 0.81447, loss_test: 0.81447\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"Loading the saved variable to the current session.\")\n",
    "saver.restore(sess=sess, save_path=save_path)\n",
    "\n",
    "auc_test, loss_test = evaluate(sess, is_train=False)\n",
    "print (\"auc_test: {0:.5}, loss_test: {0:.5}\".format(auc_test, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden_layer_1/rnn/lstm_cell/kernel:0' shape=(324, 800) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden_layer_1/rnn/lstm_cell/bias:0' shape=(800,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden_layer_2/rnn/lstm_cell/kernel:0' shape=(250, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden_layer_2/rnn/lstm_cell/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'W_yh:0' shape=(50, 124) dtype=float32_ref>,\n",
       " <tf.Variable 'b_yh:0' shape=(124,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "#http://bokeh.pydata.org/en/0.10.0/docs/gallery/cat_heatmap_chart.html\n",
    "\n",
    "def plot_heatmap(data, x_labels, y_labels, second_x_labels=None):\n",
    "#     plt.figure(figsize=(40,100))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n",
    "    \n",
    "    # Format\n",
    "    fig = plt.gcf()\n",
    "    \n",
    "    # turn off the frame\n",
    "    ax.set_frame_on(False)\n",
    "    \n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(len(x_labels)) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(len(y_labels)) + 0.5, minor=False)\n",
    "    \n",
    "    # want a more natural, table-like display\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    "    \n",
    "    # set the label\n",
    "    ax.set_xticklabels(x_labels, minor=False)\n",
    "    ax.set_yticklabels(y_labels, minor=False)\n",
    "    ax.set_xlabel(\"the skill id answered at the time step\")\n",
    "    ax.set_ylabel(\"the skill id of the output layer\")\n",
    "\n",
    "    fig.set_size_inches(15, 5)\n",
    "    \n",
    "    # second axis label\n",
    "    if second_x_labels != None:\n",
    "        ax2 = ax.twiny()\n",
    "        ax2.set_xticks(np.arange(len(second_x_labels)) + 0.5, minor=False)\n",
    "        ax2.set_xticklabels(second_x_labels)\n",
    "        ax2.set_xlabel(\"Correct Label\")\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 25, 44, 47, 54, 62, 63, 90, 109, 140, 159, 161, 175, 184, 200, 204, 217, 220, 225, 226, 238, 248, 249, 252, 266, 279, 297, 301, 312, 323, 326, 334, 354, 366, 371, 376, 390, 401, 403, 414, 416, 436, 447, 451, 452, 468, 472, 474, 488, 489, 495, 512, 527, 529, 532, 538, 541, 557, 558, 572, 573, 575, 583, 590, 592, 597, 599, 602, 618, 620, 632, 637, 638, 640, 646, 650, 656, 657, 658, 674, 677, 690, 696, 706, 720, 721, 727, 731, 759, 764, 773, 779]\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "for i in range(len(students_test)):\n",
    "    student = students_test[i]\n",
    "    num_question_answered = student[0]\n",
    "    question_ids_answered = np.sort(np.array([int(qid) for qid in set(student[1]) if qid != -1]))\n",
    "    num_distict_question = len(question_ids_answered)\n",
    "    \n",
    "    if 50 >= num_question_answered >= 20 and 10 >= num_distict_question >= 5:\n",
    "        targets.append(i)\n",
    "    \n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "['37', '37', '41', '36', '41', '36', '37', '37', '72', '37', '72', '37', '41', '36', '41', '36', '41', '36', '41', '36', '41', '36', '41', '36', '41', '36', '36', '36', '37', '37', '41', '36', '70']\n",
      "['1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "sid = targets[0]\n",
    "student = students_test[sid]\n",
    "num_question_answered = student[0]\n",
    "question_ids_answered = np.sort(np.array([int(qid) for qid in set(student[1]) if qid != -1]))\n",
    "output_layer = get_student_output_layer(sess, student)\n",
    "output_layer = output_layer[:num_question_answered, question_ids_answered]\n",
    "output_layer = np.transpose(output_layer)\n",
    "\n",
    "question_seq = student[1][:num_question_answered]\n",
    "correct_seq = student[2][:num_question_answered]\n",
    "\n",
    "print(num_question_answered),\n",
    "print(question_seq)\n",
    "print(correct_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFaCAYAAABMlf9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7CG+QkxKP6cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x216890abf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_heatmap(output_layer, question_seq, question_ids_answered, correct_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(student[1][:num_question_answered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(y_pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = []\n",
    "for i in range(len(students_test)):\n",
    "    student = students_test[i]\n",
    "    num_question_answered = student[0]\n",
    "    question_ids_answered = np.sort(np.array([int(qid) for qid in set(student[1]) if qid != -1]))\n",
    "    num_distict_question = len(question_ids_answered)\n",
    "    \n",
    "    if 50 >= num_question_answered >= 20 and 10 >= num_distict_question >= 5:\n",
    "        targets.append(i)\n",
    "    \n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
