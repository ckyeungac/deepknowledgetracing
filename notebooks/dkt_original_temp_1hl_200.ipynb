{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import load_train_test\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# specify the gpu device\n",
    "# import os\n",
    "# from Tools.utils import _make_dir, load_options\n",
    "# options = load_options('options.json')\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"OCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "SPLIT_MSG=\"***********\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'\n",
    "train_file = 'problem_id_train.csv'\n",
    "test_file = 'problem_id_test.csv'\n",
    "train_path= os.path.join(DATA_DIR, train_file)\n",
    "test_path = os.path.join(DATA_DIR, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./data/problem_id_train.csv\n",
      "9990 lines was read\n",
      "max_num_problems_answered: 1295\n",
      "num_problems: 17751\n",
      "The number of students is 3103\n",
      "Finish reading data.\n",
      "Reading ./data/problem_id_test.csv\n",
      "2499 lines was read\n",
      "max_num_problems_answered: 1169\n",
      "num_problems: 17748\n",
      "The number of students is 781\n",
      "Finish reading data.\n"
     ]
    }
   ],
   "source": [
    "students_train, students_test, max_num_steps, num_problems = load_train_test(train_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model\n",
    "\n",
    "### Placeholder Explanation\n",
    "X is the one-hot encoded input sequence of a student.\n",
    "y is the one-hot encoded correct sequence of a student.\n",
    "\n",
    "For example, the student i has a seq [1, 3, 1, 1, 2] with correct map [0, 1, 1, 1, 0]. The X_seq will be one hot encoded as:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The X_corr map will be one hot encoded as:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&0&0&0&0\\\\\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Then, it will be concatenated into $X^i$:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc|ccccc}\n",
    "        0&1&0&0&0&0&0&0&0&0\\\\\n",
    "        0&0&0&1&0&0&0&0&1&0\\\\\n",
    "        0&1&0&0&0&0&1&0&0&0\\\\\n",
    "        0&1&0&0&0&0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The last question '2' is not used in the $X^i$ because it is the last record that the student has and therefore used in $y$.\n",
    "So, $y$ would be seq [3, 1, 1, 2] with corr map [1, 1, 1, 0]\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_corr_to_onehot(seq, corr, num_steps, num_problems):\n",
    "    seq_oh = tf.one_hot(seq, depth=num_problems)\n",
    "    seq_oh_flat = tf.reshape(seq_oh, [-1, num_problems])\n",
    "    \n",
    "    # element-wise multiplication between Matrix and Vector\n",
    "    # the i-th column of Matrixelement-wisedly multiply the i-th element in the Vector\n",
    "    corr_flat = tf.reshape(corr, [-1])\n",
    "    corr_mat = tf.multiply(tf.transpose(seq_oh_flat), tf.cast(corr_flat, dtype=tf.float32))\n",
    "    corr_mat = tf.transpose(corr_mat)\n",
    "    corr_mat = tf.reshape(corr_mat, shape=[-1, num_steps, num_problems])\n",
    "    \n",
    "    concat = tf.concat([seq_oh, corr_mat], axis=2)\n",
    "    \n",
    "    return seq_oh, corr_mat, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    length = tf.reduce_sum(used, 1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# network configuration\n",
    "batch_size = 16\n",
    "max_num_steps = max_num_steps - 1\n",
    "num_problems = num_problems\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "inputs_seq = tf.placeholder(tf.int32, [None, max_num_steps])\n",
    "inputs_corr = tf.placeholder(tf.int32, [None, max_num_steps])\n",
    "X_seq, X_corr, X = seq_corr_to_onehot(inputs_seq, inputs_corr, max_num_steps, num_problems)\n",
    "\n",
    "targets_seq = tf.placeholder(tf.int32, [None, max_num_steps])\n",
    "targets_corr = tf.placeholder(tf.int32, [None, max_num_steps])\n",
    "y_seq, y_corr, _ = seq_corr_to_onehot(targets_seq, targets_corr, max_num_steps, num_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1294), Dimension(35502)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build up the network\n",
    "hl1_size = 200\n",
    "sequence_length = length(X_seq)\n",
    "\n",
    "with tf.variable_scope('hidden_layer_1'):\n",
    "    hl1_cell = tf.contrib.rnn.LSTMCell(num_units=hl1_size)\n",
    "    hl1_cell = tf.contrib.rnn.DropoutWrapper(hl1_cell, output_keep_prob=keep_prob)\n",
    "    hl1_output, hl1_state = tf.nn.dynamic_rnn(\n",
    "        hl1_cell,\n",
    "        X,\n",
    "        dtype=tf.float32,\n",
    "#         sequence_length=sequence_length\n",
    "    )\n",
    "\n",
    "last_layer_size = hl1_size\n",
    "last_layer_output, last_layer_state = hl1_output, hl1_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this code block calculate the loss using tf.gather_nd\n",
    "W_yh = tf.Variable(tf.random_normal([last_layer_size, num_problems]), name=\"W_yh\")\n",
    "b_yh = tf.Variable(tf.constant(0.1, shape=[num_problems,]), name=\"b_yh\")\n",
    "\n",
    "last_layer_output_flat = tf.reshape(last_layer_output, [-1, last_layer_size])\n",
    "logits_flat = tf.matmul(last_layer_output_flat, W_yh) + b_yh\n",
    "preds_flat = tf.sigmoid(logits_flat)\n",
    "y_seq_flat = tf.cast(tf.reshape(y_seq, [-1, num_problems]), dtype=tf.float32)\n",
    "y_corr_flat = tf.cast(tf.reshape(y_corr, [-1, num_problems]), dtype=tf.float32)\n",
    "\n",
    "# get the indices where they are not equal to 0\n",
    "# the indices implies that a student has answered the question in the time step\n",
    "# and thereby exclude those time step that the student hasn't answered.\n",
    "target_indices = tf.where(tf.not_equal(y_seq_flat, 0))\n",
    "target_logits = tf.gather_nd(logits_flat, target_indices)\n",
    "target_preds = tf.gather_nd(preds_flat, target_indices)\n",
    "target_labels = tf.gather_nd(y_corr_flat, target_indices)\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=target_logits, \n",
    "                                               labels=target_labels)\n",
    "total_loss = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize(sess, print_loss=False):    \n",
    "    students = students_train\n",
    "    \n",
    "    best_test_auc = 0\n",
    "    best_epoch_idx = 0\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        \n",
    "        num_students = 10\n",
    "        num_students = len(students) \n",
    "\n",
    "        loss_train = 0\n",
    "        iteration = 1\n",
    "        \n",
    "        for batch_idx in range(0, num_students, batch_size):\n",
    "            start_idx = batch_idx\n",
    "            end_idx = min(num_students, batch_idx+batch_size)\n",
    "            \n",
    "            new_batch_size = end_idx - start_idx\n",
    "            \n",
    "            inputs_seq_batch = np.array([tup[1][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "            inputs_corr_batch = np.array([tup[2][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "            \n",
    "            y_seq_batch = np.array([tup[1][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "            y_corr_batch = np.array([tup[2][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "\n",
    "            _optimizer, _target_preds, _target_labels, _total_loss = sess.run(\n",
    "                    [optimizer, target_preds, target_labels, total_loss],\n",
    "                    feed_dict={\n",
    "                    inputs_seq: inputs_seq_batch,\n",
    "                    inputs_corr: inputs_corr_batch,\n",
    "                    targets_seq: y_seq_batch,\n",
    "                    targets_corr: y_corr_batch,\n",
    "                    keep_prob: 0.5,\n",
    "                })\n",
    "            \n",
    "            y_pred += [p for p in _target_preds]\n",
    "            y_true += [t for t in _target_labels]\n",
    "            loss_train = (iteration-1)/(iteration) * loss_train + _total_loss/iteration\n",
    "            iteration+=1\n",
    "        \n",
    "        # Print training information        \n",
    "        fpr, tpr, thres = roc_curve(y_true, y_pred, pos_label=1)\n",
    "        auc_train = auc(fpr, tpr)\n",
    "        print('Epoch {0:>4}, Train AUC: {1:.5}, Train Loss: {2:.5}'.format(epoch_idx+1, auc_train, loss_train))\n",
    "        \n",
    "        # evaluate on the test set\n",
    "        auc_test, loss_test = evaluate(sess, is_train=False)\n",
    "        test_msg = \"Epoch {0:>4}, Test AUC: {1:.5}, Test Loss: {2:.5}\".format(epoch_idx+1, auc_test, loss_test)\n",
    "        if auc_test > best_test_auc:\n",
    "            test_msg += \"*\"\n",
    "            best_epoch_idx = epoch_idx\n",
    "            best_test_auc = auc_test\n",
    "            saver.save(sess=sess, save_path=save_path)\n",
    "        print(test_msg)\n",
    "        print(SPLIT_MSG)        \n",
    "        # quit the training if there is no improve in AUC for 20 epochs.\n",
    "        if epoch_idx - best_epoch_idx >= 20:\n",
    "            print(\"No improvement shown in 20 epochs. Quit Training.\")\n",
    "            break\n",
    "    print(\"The best testing result occured at: {0}-th epoch, with testing AUC: {1:.5}\".format(best_epoch_idx, best_test_auc))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def evaluate(sess, is_train=False):    \n",
    "    if is_train:\n",
    "        students = students_train\n",
    "    else:\n",
    "        students = students_test\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    iteration = 1\n",
    "    _loss = 0\n",
    "    \n",
    "    num_students = 10\n",
    "    num_students = len(students)\n",
    "    \n",
    "    for batch_idx in range(0, num_students, batch_size):\n",
    "        start_idx = batch_idx\n",
    "        end_idx = min(num_students, batch_idx+batch_size)\n",
    "\n",
    "        new_batch_size = end_idx - start_idx\n",
    "\n",
    "        inputs_seq_batch = np.array([tup[1][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "        inputs_corr_batch = np.array([tup[2][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "\n",
    "        y_seq_batch = np.array([tup[1][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "        y_corr_batch = np.array([tup[2][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "\n",
    "        _target_preds, _target_labels, _total_loss = sess.run(\n",
    "                [target_preds, target_labels, total_loss],\n",
    "                feed_dict={\n",
    "                inputs_seq: inputs_seq_batch,\n",
    "                inputs_corr: inputs_corr_batch,\n",
    "                targets_seq: y_seq_batch,\n",
    "                targets_corr: y_corr_batch,\n",
    "                keep_prob: 1,\n",
    "            })\n",
    "\n",
    "        y_pred += [p for p in _target_preds]\n",
    "        y_true += [t for t in _target_labels]\n",
    "        _loss = (iteration-1)/(iteration) * _loss + _total_loss/iteration\n",
    "        iteration+=1\n",
    "\n",
    "    fpr, tpr, thres = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    return (auc_score, _loss)\n",
    "\n",
    "def get_student_output_layer(sess, student):\n",
    "    num_steps = len(student[1]) - 1\n",
    "    shape = (1, num_steps)\n",
    "    _inputs_seq = np.array(student[1][:-1]).reshape(shape)\n",
    "    _inputs_corr = np.array(student[2][:-1]).reshape(shape)\n",
    "    \n",
    "    _y_seq = np.array(student[1][1:]).reshape(shape)\n",
    "    _y_corr = np.array(student[2][1:]).reshape(shape)\n",
    "    \n",
    "    _preds_flat = sess.run(\n",
    "        preds_flat,\n",
    "        feed_dict={\n",
    "            inputs_seq: _inputs_seq,\n",
    "            inputs_corr: _inputs_corr,\n",
    "            targets_seq: _y_seq,\n",
    "            targets_corr: _y_corr,\n",
    "            keep_prob: 1,\n",
    "        }\n",
    "    )    \n",
    "    return _preds_flat\n",
    "\n",
    "def get_student_hidden_layer(sess, student, layer_num=1):\n",
    "    _inputs_seq = np.array(tup[1][:-1])\n",
    "    _inputs_corr = np.array(tup[2][:-1])\n",
    "    \n",
    "    _y_seq = np.array(tup[1][1:])\n",
    "    _y_corr = np.array(tup[2][1:])\n",
    "    \n",
    "    feed_dict={\n",
    "            inputs_seq: _inputs_seq,\n",
    "            inputs_corr: _inputs_corr,\n",
    "            targets_seq: _y_seq,\n",
    "            targets_corr: _y_corr,\n",
    "            keep_prob: 1,\n",
    "        }\n",
    "    \n",
    "    result = None\n",
    "    if layer_num == 1:\n",
    "        result = sess.run(\n",
    "            [hl1_output],\n",
    "            feed_dict=feed_dict,\n",
    "        )\n",
    "    elif layer_num == 2:\n",
    "        result = sess.run(\n",
    "            [hl2_output],\n",
    "            feed_dict=feed_dict,\n",
    "        )\n",
    "    else:\n",
    "        print(\"layer is not available\")\n",
    "        return None\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define the tf saver\n",
    "saver = tf.train.Saver()\n",
    "save_dir = 'checkpoints/original_temp_1hl_200/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WITH_CONFIG = True\n",
    "num_epochs = 1000\n",
    "\n",
    "### Start Training\n",
    "start_time = time.time()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    try:\n",
    "        saver.restore(sess=sess, save_path=save_path)\n",
    "        print(\"Pre-trained model found, loading the previous variables\")\n",
    "    except:\n",
    "        print(\"Pre-trained model not found, train from scratch now.\")\n",
    "    optimize(sess)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"program run for: {0}s\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the saved variable to the current session.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/original/model\n",
      "auc_test: 0.80057, loss_test: 0.80057\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"Loading the saved variable to the current session.\")\n",
    "saver.restore(sess=sess, save_path=save_path)\n",
    "\n",
    "auc_test, loss_test = evaluate(sess, is_train=False)\n",
    "print (\"auc_test: {0:.5}, loss_test: {0:.5}\".format(auc_test, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden_layer_1/rnn/lstm_cell/kernel:0' shape=(448, 800) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden_layer_1/rnn/lstm_cell/bias:0' shape=(800,) dtype=float32_ref>,\n",
       " <tf.Variable 'W_yh:0' shape=(200, 124) dtype=float32_ref>,\n",
       " <tf.Variable 'b_yh:0' shape=(124,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "#http://bokeh.pydata.org/en/0.10.0/docs/gallery/cat_heatmap_chart.html\n",
    "\n",
    "def plot_heatmap(data, x_labels, y_labels, second_x_labels=None):\n",
    "#     plt.figure(figsize=(40,100))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n",
    "    \n",
    "    # Format\n",
    "    fig = plt.gcf()\n",
    "    \n",
    "    # turn off the frame\n",
    "    ax.set_frame_on(False)\n",
    "    \n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(len(x_labels)) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(len(y_labels)) + 0.5, minor=False)\n",
    "    \n",
    "    # want a more natural, table-like display\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    "    \n",
    "    # set the label\n",
    "    ax.set_xticklabels(x_labels, minor=False)\n",
    "    ax.set_yticklabels(y_labels, minor=False)\n",
    "    ax.set_xlabel(\"the skill id answered at the time step\")\n",
    "    ax.set_ylabel(\"the skill id of the output layer\")\n",
    "\n",
    "    fig.set_size_inches(15, 5)\n",
    "    \n",
    "    # second axis label\n",
    "    if second_x_labels != None:\n",
    "        ax2 = ax.twiny()\n",
    "        ax2.set_xticks(np.arange(len(second_x_labels)) + 0.5, minor=False)\n",
    "        ax2.set_xticklabels(second_x_labels)\n",
    "        ax2.set_xlabel(\"Correct Label\")\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 13, 22, 42, 43, 49, 55, 83, 89, 97, 99, 104, 111, 118, 126, 132, 137, 154, 169, 171, 174, 176, 179, 183, 193, 227, 235, 239, 261, 295, 308, 309, 321, 328, 348, 350, 352, 363, 364, 366, 375, 376, 385, 387, 388, 399, 401, 402, 406, 421, 422, 433, 477, 481, 486, 487, 493, 495, 498, 532, 534, 537, 540, 551, 575, 582, 607, 612, 613, 633, 634, 646, 647, 657, 660, 681, 686, 687, 691, 697, 707, 715, 726, 727, 740, 741, 754, 757, 759, 773, 774, 777]\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "for i in range(len(students_test)):\n",
    "    student = students_test[i]\n",
    "    num_question_answered = student[0]\n",
    "    question_ids_answered = np.sort(np.array([int(qid) for qid in set(student[1]) if qid != -1]))\n",
    "    num_distict_question = len(question_ids_answered)\n",
    "    \n",
    "    if 50 >= num_question_answered >= 20 and 10 >= num_distict_question >= 5:\n",
    "        targets.append(i)\n",
    "    \n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['76', '76', '76', '76', '76', '49', '80', '81', '55', '76', '49', '80', '81', '55', '76', '49', '80', '81', '55', '76']\n",
      "['1', '0', '0', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "sid = targets[0]\n",
    "student = students_test[sid]\n",
    "num_question_answered = student[0]\n",
    "question_ids_answered = np.sort(np.array([int(qid) for qid in set(student[1]) if qid != -1]))\n",
    "output_layer = get_student_output_layer(sess, student)\n",
    "output_layer = output_layer[:num_question_answered, question_ids_answered]\n",
    "output_layer = np.transpose(output_layer)\n",
    "\n",
    "question_seq = student[1][:num_question_answered]\n",
    "correct_seq = student[2][:num_question_answered]\n",
    "\n",
    "print(num_question_answered),\n",
    "print(question_seq)\n",
    "print(correct_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFaCAYAAABMlf9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24595eb22e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_heatmap(output_layer, student[1][:num_question_answered], question_ids_answered, student[2][:num_question_answered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
