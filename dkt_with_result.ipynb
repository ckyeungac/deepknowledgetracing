{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'\n",
    "#train_file = os.path.join(DATA_DIR, 'builder_train.csv')\n",
    "#test_file = os.path.join(DATA_DIR, 'builder_test.csv')\n",
    "train_file = os.path.join(DATA_DIR, '0910_b_train.csv')\n",
    "test_file = os.path.join(DATA_DIR, '0910_b_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data_from_csv(filename):\n",
    "    rows = []\n",
    "    max_num_problems_answered = 0\n",
    "    num_problems = 0\n",
    "    \n",
    "    print(\"Reading {0}\".format(filename))\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "    print(\"{0} lines was read\".format(len(rows)))\n",
    "    \n",
    "    # tuples stores the student answering sequence as \n",
    "    # ([num_problems_answered], [problem_ids], [is_corrects])\n",
    "    tuples = []\n",
    "    for i in range(0, len(rows), 3):\n",
    "        # numbers of problem a student answered\n",
    "        num_problems_answered = int(rows[i][0])\n",
    "        \n",
    "        # only keep student with at least 3 records.\n",
    "        if num_problems_answered < 3:\n",
    "            continue\n",
    "        \n",
    "        problem_ids = rows[i+1]\n",
    "        is_corrects = rows[i+2]\n",
    "        \n",
    "        invalid_ids_loc = [i for i, pid in enumerate(problem_ids) if pid=='']        \n",
    "        for invalid_loc in invalid_ids_loc:\n",
    "            del problem_ids[invalid_loc]\n",
    "            del is_corrects[invalid_loc]\n",
    "        \n",
    "        tup =(num_problems_answered, problem_ids, is_corrects)\n",
    "        tuples.append(tup)\n",
    "        \n",
    "        if max_num_problems_answered < num_problems_answered:\n",
    "            max_num_problems_answered = num_problems_answered\n",
    "        \n",
    "        pid = max(int(pid) for pid in problem_ids if pid!='')\n",
    "        if num_problems < pid:\n",
    "            num_problems = pid\n",
    "    # add 1 to num_problems because 0 is in the pid\n",
    "    num_problems+=1\n",
    "\n",
    "    #shuffle the tuple\n",
    "    random.shuffle(tuples)\n",
    "\n",
    "    print (\"max_num_problems_answered:\", max_num_problems_answered)\n",
    "    print (\"num_problems:\", num_problems)\n",
    "    print(\"The number of students is {0}\".format(len(tuples)))\n",
    "    print(\"Finish reading data.\")\n",
    "    \n",
    "    return tuples, max_num_problems_answered, num_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding(student_tuple, target_length):\n",
    "    num_problems_answered = student_tuple[0]\n",
    "    question_seq = student_tuple[1]\n",
    "    question_corr = student_tuple[2]\n",
    "    \n",
    "    pad_length = target_length - num_problems_answered\n",
    "    question_seq += [-1]*pad_length\n",
    "    question_corr += [0]*pad_length\n",
    "    \n",
    "    new_student_tuple = (num_problems_answered, question_seq, question_corr)\n",
    "    return new_student_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./data/0910_b_train.csv\n",
      "10116 lines was read\n",
      "max_num_problems_answered: 1219\n",
      "num_problems: 124\n",
      "The number of students is 3134\n",
      "Finish reading data.\n",
      "Reading ./data/0910_b_test.csv\n",
      "2532 lines was read\n",
      "max_num_problems_answered: 1062\n",
      "num_problems: 124\n",
      "The number of students is 786\n",
      "Finish reading data.\n"
     ]
    }
   ],
   "source": [
    "students_train, max_num_problems_answered_train, num_problems_train = \\\n",
    "read_data_from_csv(train_file)\n",
    "\n",
    "students_train = [padding(student_tuple, max_num_problems_answered_train) \n",
    "                  for student_tuple in students_train]\n",
    "\n",
    "students_test, max_num_problems_answered_test, num_problems_test = \\\n",
    "read_data_from_csv(test_file)\n",
    "\n",
    "students_test = [padding(student_tuple, max_num_problems_answered_train) \n",
    "                  for student_tuple in students_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model\n",
    "\n",
    "### Placeholder Explanation\n",
    "X is the one-hot encoded input sequence of a student.\n",
    "y is the one-hot encoded correct sequence of a student.\n",
    "\n",
    "For example, the student i has a seq [1, 3, 1, 1, 2] with correct map [0, 1, 1, 1, 0]. The X_seq will be one hot encoded as:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The X_corr map will be one hot encoded as:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&0&0&0&0\\\\\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Then, it will be concatenated into $X^i$:\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc|ccccc}\n",
    "        0&1&0&0&0&0&0&0&0&0\\\\\n",
    "        0&0&0&1&0&0&0&0&1&0\\\\\n",
    "        0&1&0&0&0&0&1&0&0&0\\\\\n",
    "        0&1&0&0&0&0&1&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The last question '2' is not used in the $X^i$ because it is the last record that the student has and therefore used in $y$.\n",
    "So, $y$ would be seq [3, 1, 1, 2] with corr map [1, 1, 1, 0]\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{array}{ccccc}\n",
    "        0&0&0&1&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&1&0&0&0\\\\\n",
    "        0&0&0&0&0\\\\\n",
    "    \\end{array}\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_corr_to_onehot(seq, corr, num_steps, num_problems):\n",
    "    seq_oh = tf.one_hot(seq, depth=num_problems)\n",
    "    seq_oh_flat = tf.reshape(seq_oh, [-1, num_problems])\n",
    "    \n",
    "    # element-wise multiplication between Matrix and Vector\n",
    "    # the i-th column of Matrixelement-wisedly multiply the i-th element in the Vector\n",
    "    corr_flat = tf.reshape(corr, [-1])\n",
    "    corr_mat = tf.multiply(tf.transpose(seq_oh_flat), tf.cast(corr_flat, dtype=tf.float32))\n",
    "    corr_mat = tf.transpose(corr_mat)\n",
    "    corr_mat = tf.reshape(corr_mat, shape=[-1, num_steps, num_problems])\n",
    "    \n",
    "    return seq_oh, corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# network configuration\n",
    "batch_size = 32\n",
    "num_layers = 1\n",
    "state_size = 200\n",
    "num_steps = max_num_problems_answered_train-1\n",
    "num_problems = num_problems_train\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "inputs_seq = tf.placeholder(tf.int32, [None, num_steps])\n",
    "inputs_corr = tf.placeholder(tf.int32, [None, num_steps])\n",
    "X_seq, X_corr = seq_corr_to_onehot(inputs_seq, inputs_corr, num_steps, num_problems)\n",
    "X = tf.concat([X_seq, X_corr], axis=2, name='X')\n",
    "X = tf.cast(X, dtype=tf.float32)\n",
    "\n",
    "targets_seq = tf.placeholder(tf.int32, [None, num_steps])\n",
    "targets_corr = tf.placeholder(tf.int32, [None, num_steps])\n",
    "y_seq, y_corr = seq_corr_to_onehot(targets_seq, targets_corr, num_steps, num_problems)\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, None, state_size])\n",
    "state_per_layer_list  = tf.unstack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple([tf.contrib.rnn.LSTMStateTuple(\n",
    "            state_per_layer_list[idx][0],\n",
    "            state_per_layer_list[idx][1]\n",
    "        ) for idx in range(num_layers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Configuration\n",
    "There are basically 2 elements needed to construct the LSTM network\n",
    "1. The cell, and\n",
    "2. The rnn structure.\n",
    "\n",
    "The cell is defined via the tf.contrib.rnn library. It supports the multilayer RNN as well. \n",
    "\n",
    "The RNN is defined via the tf.nn.dynamic_rnn. It is parameterized by the cell defined, the input X, and a initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the states series is:\n",
      " Tensor(\"rnn/rnn/transpose:0\", shape=(?, 1218, 200), dtype=float32)\n",
      "\n",
      "the current_state is:\n",
      " (LSTMStateTuple(c=<tf.Tensor 'rnn/rnn/while/Exit_2:0' shape=(?, 200) dtype=float32>, h=<tf.Tensor 'rnn/rnn/while/Exit_3:0' shape=(?, 200) dtype=float32>),)\n"
     ]
    }
   ],
   "source": [
    "# build up the network\n",
    "with tf.variable_scope('cell'):\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units=state_size,\n",
    "                                   forget_bias=1.0,\n",
    "                                   state_is_tuple=True)\n",
    "    \n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                        output_keep_prob=keep_prob)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell]*num_layers, state_is_tuple=True)\n",
    "\n",
    "with tf.variable_scope('rnn'):\n",
    "    states_series, current_state = tf.nn.dynamic_rnn(cell, \n",
    "                                                    X,\n",
    "                                                    initial_state=rnn_tuple_state,\n",
    "                                                    time_major=False)\n",
    "\n",
    "print(\"the states series is:\\n\", states_series)\n",
    "print(\"\\nthe current_state is:\\n\", current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # this code block calculate the loss using reduce_sum\n",
    "# W_yh = tf.Variable(tf.random_normal([state_size, output_size]), name=\"W_yh\")\n",
    "# b_yh = tf.Variable(tf.constant(0.1, shape=[output_size,]), name=\"b_yh\")\n",
    "\n",
    "# states_series = tf.reshape(states_series, [-1, state_size])\n",
    "# logits_flat = tf.matmul(states_series, W_yh) + b_yh\n",
    "# y_seq_flat = tf.cast(tf.reshape(y_seq, [-1, output_size]), dtype=tf.float32)\n",
    "# y_corr_flat = tf.cast(tf.reshape(y_corr, [-1, output_size]), dtype=tf.float32)\n",
    "\n",
    "# target_logits = tf.multiply(logits_flat, y_seq_flat)\n",
    "# target_logits = tf.reduce_sum(target_logits, axis=1)\n",
    "\n",
    "# target_labels = tf.multiply(y_corr_flat, y_seq_flat)\n",
    "# target_labels = tf.reduce_sum(target_labels, axis=1)\n",
    "\n",
    "# loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=target_logits, \n",
    "#                                                labels=target_labels)\n",
    "# total_loss = tf.reduce_mean(loss)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code block calculate the loss using tf.gather_nd\n",
    "W_yh = tf.Variable(tf.random_normal([state_size, num_problems]), name=\"W_yh\")\n",
    "b_yh = tf.Variable(tf.constant(0.1, shape=[num_problems,]), name=\"b_yh\")\n",
    "\n",
    "states_series = tf.reshape(states_series, [-1, state_size])\n",
    "logits_flat = tf.matmul(states_series, W_yh) + b_yh\n",
    "y_seq_flat = tf.cast(tf.reshape(y_seq, [-1, num_problems]), dtype=tf.float32)\n",
    "y_corr_flat = tf.cast(tf.reshape(y_corr, [-1, num_problems]), dtype=tf.float32)\n",
    "\n",
    "# get the indices where they are not equal to 0\n",
    "# the indices implies that a student has answered the question in the time step\n",
    "# and thereby exclude those time step that the student hasn't answered.\n",
    "target_indices = tf.where(tf.not_equal(y_seq_flat, 0))\n",
    "target_logits = tf.gather_nd(logits_flat, target_indices)\n",
    "target_labels = tf.gather_nd(y_corr_flat, target_indices)\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=target_logits, \n",
    "                                               labels=target_labels)\n",
    "total_loss = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize(sess):\n",
    "    students = students_train\n",
    "    \n",
    "    # update the network configuration\n",
    "    global num_steps\n",
    "    num_steps = max_num_problems_answered_train - 1\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        num_students = len(students)\n",
    "        iteration = 0\n",
    "        for batch_idx in range(0, num_students, batch_size):\n",
    "            start_idx = batch_idx\n",
    "            end_idx = min(num_students, batch_idx+batch_size)\n",
    "            \n",
    "            new_batch_size = end_idx - start_idx\n",
    "            _current_state = np.zeros((num_layers, 2, new_batch_size, state_size))\n",
    "            \n",
    "            inputs_seq_batch = np.array([tup[1][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "            inputs_corr_batch = np.array([tup[2][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "            \n",
    "            y_seq_batch = np.array([tup[1][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "            y_corr_batch = np.array([tup[2][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "\n",
    "            _optimizer, _current_state, = sess.run(\n",
    "                    [optimizer, current_state],\n",
    "                    feed_dict={\n",
    "                    inputs_seq: inputs_seq_batch,\n",
    "                    inputs_corr: inputs_corr_batch,\n",
    "                    targets_seq: y_seq_batch,\n",
    "                    targets_corr: y_corr_batch,\n",
    "                    init_state: _current_state,\n",
    "                    keep_prob: 0.5,\n",
    "                })\n",
    "            \n",
    "            if iteration%10 == 0:\n",
    "                _total_loss= sess.run(total_loss,\n",
    "                    feed_dict={\n",
    "                    inputs_seq: inputs_seq_batch,\n",
    "                    inputs_corr: inputs_corr_batch,\n",
    "                    targets_seq: y_seq_batch,\n",
    "                    targets_corr: y_corr_batch,\n",
    "                    init_state: _current_state,\n",
    "                    keep_prob: 1,\n",
    "                })\n",
    "                print(\"Epoch {0:>4}, iteration {1:>4}, batch loss value: {2:.5}\".format(epoch_idx, iteration, _total_loss))\n",
    "            \n",
    "            iteration+=1\n",
    "        auc_train = evaluate(sess, is_train=True)\n",
    "        auc_test = evaluate(sess, is_train=False)\n",
    "        print(\"Epoch {0:>4}, Training AUC: {1:.5}, Testing AUC: {2:.5}\".format(epoch_idx, auc_train, auc_test))\n",
    "        \n",
    "\n",
    "def evaluate(sess, is_train=False):\n",
    "    global num_steps\n",
    "    \n",
    "    if is_train:\n",
    "        students = students_train\n",
    "        num_steps = max_num_problems_answered_train\n",
    "    else:\n",
    "        students = students_test\n",
    "        num_steps = max_num_problems_answered_test\n",
    "\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    num_students = len(students)\n",
    "    for batch_idx in range(0, num_students, batch_size):\n",
    "        start_idx = batch_idx\n",
    "        end_idx = min(num_students, batch_idx+batch_size)\n",
    "\n",
    "        new_batch_size = end_idx - start_idx\n",
    "        _current_state = np.zeros((num_layers, 2, new_batch_size, state_size))\n",
    "\n",
    "        inputs_seq_batch = np.array([tup[1][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "        inputs_corr_batch = np.array([tup[2][:-1] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "\n",
    "        y_seq_batch = np.array([tup[1][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "        y_corr_batch = np.array([tup[2][1:] for tup in students[start_idx:end_idx]], dtype=np.int32)\n",
    "\n",
    "        _target_logits, _target_labels = sess.run(\n",
    "                [target_logits, target_labels],\n",
    "                feed_dict={\n",
    "                inputs_seq: inputs_seq_batch,\n",
    "                inputs_corr: inputs_corr_batch,\n",
    "                targets_seq: y_seq_batch,\n",
    "                targets_corr: y_corr_batch,\n",
    "                init_state: _current_state,\n",
    "                keep_prob: 1,\n",
    "            })\n",
    "\n",
    "        y_pred += [p for p in _target_logits]\n",
    "        y_true += [t for t in _target_labels]\n",
    "\n",
    "    fpr, tpr, thres = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, iteration    0, batch loss value: 0.6662\n",
      "Epoch    0, iteration   10, batch loss value: 0.64877\n",
      "Epoch    0, iteration   20, batch loss value: 0.64473\n",
      "Epoch    0, iteration   30, batch loss value: 0.62036\n",
      "Epoch    0, iteration   40, batch loss value: 0.57655\n",
      "Epoch    0, iteration   50, batch loss value: 0.59314\n",
      "Epoch    0, iteration   60, batch loss value: 0.58415\n",
      "Epoch    0, iteration   70, batch loss value: 0.59103\n",
      "Epoch    0, iteration   80, batch loss value: 0.55709\n",
      "Epoch    0, iteration   90, batch loss value: 0.58463\n",
      "Epoch    0, Training AUC: 0.75506, Testing AUC: 0.7407\n",
      "Epoch    1, iteration    0, batch loss value: 0.55538\n",
      "Epoch    1, iteration   10, batch loss value: 0.57281\n",
      "Epoch    1, iteration   20, batch loss value: 0.55722\n",
      "Epoch    1, iteration   30, batch loss value: 0.56395\n",
      "Epoch    1, iteration   40, batch loss value: 0.52884\n",
      "Epoch    1, iteration   50, batch loss value: 0.55677\n",
      "Epoch    1, iteration   60, batch loss value: 0.55364\n",
      "Epoch    1, iteration   70, batch loss value: 0.55145\n",
      "Epoch    1, iteration   80, batch loss value: 0.52739\n",
      "Epoch    1, iteration   90, batch loss value: 3.1403\n",
      "Epoch    1, Training AUC: 0.78033, Testing AUC: 0.76954\n",
      "Epoch    2, iteration    0, batch loss value: 0.53983\n",
      "Epoch    2, iteration   10, batch loss value: 0.55448\n",
      "Epoch    2, iteration   20, batch loss value: 0.53203\n",
      "Epoch    2, iteration   30, batch loss value: 0.55189\n",
      "Epoch    2, iteration   40, batch loss value: 0.50831\n",
      "Epoch    2, iteration   50, batch loss value: 0.53838\n",
      "Epoch    2, iteration   60, batch loss value: 0.53352\n",
      "Epoch    2, iteration   70, batch loss value: 0.53206\n",
      "Epoch    2, iteration   80, batch loss value: 0.5091\n",
      "Epoch    2, iteration   90, batch loss value: 0.53333\n",
      "Epoch    2, Training AUC: 0.79406, Testing AUC: 0.78276\n",
      "Epoch    3, iteration    0, batch loss value: 0.52922\n",
      "Epoch    3, iteration   10, batch loss value: 0.54699\n",
      "Epoch    3, iteration   20, batch loss value: 0.5189\n",
      "Epoch    3, iteration   30, batch loss value: 0.54388\n",
      "Epoch    3, iteration   40, batch loss value: 0.49647\n",
      "Epoch    3, iteration   50, batch loss value: 0.52809\n",
      "Epoch    3, iteration   60, batch loss value: 0.52244\n",
      "Epoch    3, iteration   70, batch loss value: 0.5205\n",
      "Epoch    3, iteration   80, batch loss value: 0.49533\n",
      "Epoch    3, iteration   90, batch loss value: 0.52179\n",
      "Epoch    3, Training AUC: 0.80315, Testing AUC: 0.79058\n",
      "Epoch    4, iteration    0, batch loss value: 0.52155\n",
      "Epoch    4, iteration   10, batch loss value: 0.53886\n",
      "Epoch    4, iteration   20, batch loss value: 0.50604\n",
      "Epoch    4, iteration   30, batch loss value: 0.53173\n",
      "Epoch    4, iteration   40, batch loss value: 0.48509\n",
      "Epoch    4, iteration   50, batch loss value: 0.5189\n",
      "Epoch    4, iteration   60, batch loss value: 0.5119\n",
      "Epoch    4, iteration   70, batch loss value: 0.51029\n",
      "Epoch    4, iteration   80, batch loss value: 0.48562\n",
      "Epoch    4, iteration   90, batch loss value: 0.51602\n",
      "Epoch    4, Training AUC: 0.81082, Testing AUC: 0.79748\n",
      "Epoch    5, iteration    0, batch loss value: 0.52731\n",
      "Epoch    5, iteration   10, batch loss value: 0.5271\n",
      "Epoch    5, iteration   20, batch loss value: 0.51777\n",
      "Epoch    5, iteration   30, batch loss value: 0.52976\n",
      "Epoch    5, iteration   40, batch loss value: 0.47551\n",
      "Epoch    5, iteration   50, batch loss value: 0.51071\n",
      "Epoch    5, iteration   60, batch loss value: 0.50402\n",
      "Epoch    5, iteration   70, batch loss value: 0.50738\n",
      "Epoch    5, iteration   80, batch loss value: 0.47847\n",
      "Epoch    5, iteration   90, batch loss value: 0.50421\n",
      "Epoch    5, Training AUC: 0.81582, Testing AUC: 0.8008\n",
      "Epoch    6, iteration    0, batch loss value: 0.51173\n",
      "Epoch    6, iteration   10, batch loss value: 0.52255\n",
      "Epoch    6, iteration   20, batch loss value: 0.49565\n",
      "Epoch    6, iteration   30, batch loss value: 0.51888\n",
      "Epoch    6, iteration   40, batch loss value: 0.47055\n",
      "Epoch    6, iteration   50, batch loss value: 0.50621\n",
      "Epoch    6, iteration   60, batch loss value: 0.49635\n",
      "Epoch    6, iteration   70, batch loss value: 0.50095\n",
      "Epoch    6, iteration   80, batch loss value: 0.47243\n",
      "Epoch    6, iteration   90, batch loss value: 0.49764\n",
      "Epoch    6, Training AUC: 0.82061, Testing AUC: 0.80528\n",
      "Epoch    7, iteration    0, batch loss value: 0.51126\n",
      "Epoch    7, iteration   10, batch loss value: 0.51631\n",
      "Epoch    7, iteration   20, batch loss value: 0.50614\n",
      "Epoch    7, iteration   30, batch loss value: 0.51101\n",
      "Epoch    7, iteration   40, batch loss value: 0.46475\n",
      "Epoch    7, iteration   50, batch loss value: 0.49996\n",
      "Epoch    7, iteration   60, batch loss value: 0.48881\n",
      "Epoch    7, iteration   70, batch loss value: 0.52256\n",
      "Epoch    7, iteration   80, batch loss value: 0.46671\n",
      "Epoch    7, iteration   90, batch loss value: 0.4901\n",
      "Epoch    7, Training AUC: 0.82288, Testing AUC: 0.80584\n",
      "Epoch    8, iteration    0, batch loss value: 0.50314\n",
      "Epoch    8, iteration   10, batch loss value: 0.51417\n",
      "Epoch    8, iteration   20, batch loss value: 0.48303\n",
      "Epoch    8, iteration   30, batch loss value: 0.51087\n",
      "Epoch    8, iteration   40, batch loss value: 0.45792\n",
      "Epoch    8, iteration   50, batch loss value: 0.49505\n",
      "Epoch    8, iteration   60, batch loss value: 0.48445\n",
      "Epoch    8, iteration   70, batch loss value: 0.48402\n",
      "Epoch    8, iteration   80, batch loss value: 0.46141\n",
      "Epoch    8, iteration   90, batch loss value: 0.48756\n",
      "Epoch    8, Training AUC: 0.8266, Testing AUC: 0.80985\n",
      "Epoch    9, iteration    0, batch loss value: 0.51201\n",
      "Epoch    9, iteration   10, batch loss value: 0.50887\n",
      "Epoch    9, iteration   20, batch loss value: 0.48972\n",
      "Epoch    9, iteration   30, batch loss value: 0.51471\n",
      "Epoch    9, iteration   40, batch loss value: 0.45364\n",
      "Epoch    9, iteration   50, batch loss value: 0.49219\n",
      "Epoch    9, iteration   60, batch loss value: 0.48248\n",
      "Epoch    9, iteration   70, batch loss value: 0.48496\n",
      "Epoch    9, iteration   80, batch loss value: 0.45767\n",
      "Epoch    9, iteration   90, batch loss value: 0.48344\n",
      "Epoch    9, Training AUC: 0.82903, Testing AUC: 0.81029\n",
      "Epoch   10, iteration    0, batch loss value: 0.54248\n",
      "Epoch   10, iteration   10, batch loss value: 0.50516\n",
      "Epoch   10, iteration   20, batch loss value: 0.48694\n",
      "Epoch   10, iteration   30, batch loss value: 0.50377\n",
      "Epoch   10, iteration   40, batch loss value: 0.44824\n",
      "Epoch   10, iteration   50, batch loss value: 0.48923\n",
      "Epoch   10, iteration   60, batch loss value: 0.4792\n",
      "Epoch   10, iteration   70, batch loss value: 0.47994\n",
      "Epoch   10, iteration   80, batch loss value: 0.45552\n",
      "Epoch   10, iteration   90, batch loss value: 0.47677\n",
      "Epoch   10, Training AUC: 0.83086, Testing AUC: 0.81104\n",
      "Epoch   11, iteration    0, batch loss value: 0.50258\n",
      "Epoch   11, iteration   10, batch loss value: 0.50159\n",
      "Epoch   11, iteration   20, batch loss value: 0.48509\n",
      "Epoch   11, iteration   30, batch loss value: 0.496\n",
      "Epoch   11, iteration   40, batch loss value: 0.45059\n",
      "Epoch   11, iteration   50, batch loss value: 0.48587\n",
      "Epoch   11, iteration   60, batch loss value: 0.47424\n",
      "Epoch   11, iteration   70, batch loss value: 0.47584\n",
      "Epoch   11, iteration   80, batch loss value: 0.45342\n",
      "Epoch   11, iteration   90, batch loss value: 0.48033\n",
      "Epoch   11, Training AUC: 0.83329, Testing AUC: 0.81355\n",
      "Epoch   12, iteration    0, batch loss value: 0.50662\n",
      "Epoch   12, iteration   10, batch loss value: 0.49761\n",
      "Epoch   12, iteration   20, batch loss value: 0.49005\n",
      "Epoch   12, iteration   30, batch loss value: 0.50232\n",
      "Epoch   12, iteration   40, batch loss value: 0.44203\n",
      "Epoch   12, iteration   50, batch loss value: 0.48352\n",
      "Epoch   12, iteration   60, batch loss value: 0.47193\n",
      "Epoch   12, iteration   70, batch loss value: 0.47111\n",
      "Epoch   12, iteration   80, batch loss value: 0.45005\n",
      "Epoch   12, iteration   90, batch loss value: 0.47199\n",
      "Epoch   12, Training AUC: 0.83464, Testing AUC: 0.81339\n",
      "Epoch   13, iteration    0, batch loss value: 0.49794\n",
      "Epoch   13, iteration   10, batch loss value: 0.49932\n",
      "Epoch   13, iteration   20, batch loss value: 0.48189\n",
      "Epoch   13, iteration   30, batch loss value: 0.49534\n",
      "Epoch   13, iteration   40, batch loss value: 0.43837\n",
      "Epoch   13, iteration   50, batch loss value: 0.48192\n",
      "Epoch   13, iteration   60, batch loss value: 0.47011\n",
      "Epoch   13, iteration   70, batch loss value: 0.4674\n",
      "Epoch   13, iteration   80, batch loss value: 0.45048\n",
      "Epoch   13, iteration   90, batch loss value: 0.47426\n",
      "Epoch   13, Training AUC: 0.83636, Testing AUC: 0.81442\n",
      "Epoch   14, iteration    0, batch loss value: 0.49582\n",
      "Epoch   14, iteration   10, batch loss value: 0.49109\n",
      "Epoch   14, iteration   20, batch loss value: 0.4909\n",
      "Epoch   14, iteration   30, batch loss value: 0.49183\n",
      "Epoch   14, iteration   40, batch loss value: 0.44235\n",
      "Epoch   14, iteration   50, batch loss value: 0.47873\n",
      "Epoch   14, iteration   60, batch loss value: 0.46779\n",
      "Epoch   14, iteration   70, batch loss value: 0.47318\n",
      "Epoch   14, iteration   80, batch loss value: 0.44645\n",
      "Epoch   14, iteration   90, batch loss value: 0.46653\n",
      "Epoch   14, Training AUC: 0.83763, Testing AUC: 0.81444\n",
      "Epoch   15, iteration    0, batch loss value: 0.50062\n",
      "Epoch   15, iteration   10, batch loss value: 0.48919\n",
      "Epoch   15, iteration   20, batch loss value: 0.49167\n",
      "Epoch   15, iteration   30, batch loss value: 0.48425\n",
      "Epoch   15, iteration   40, batch loss value: 0.4348\n",
      "Epoch   15, iteration   50, batch loss value: 0.47776\n",
      "Epoch   15, iteration   60, batch loss value: 0.46669\n",
      "Epoch   15, iteration   70, batch loss value: 0.45908\n",
      "Epoch   15, iteration   80, batch loss value: 0.44501\n",
      "Epoch   15, iteration   90, batch loss value: 0.46604\n",
      "Epoch   15, Training AUC: 0.83943, Testing AUC: 0.81567\n",
      "Epoch   16, iteration    0, batch loss value: 0.49742\n",
      "Epoch   16, iteration   10, batch loss value: 0.49149\n",
      "Epoch   16, iteration   20, batch loss value: 0.48388\n",
      "Epoch   16, iteration   30, batch loss value: 0.49306\n",
      "Epoch   16, iteration   40, batch loss value: 0.43311\n",
      "Epoch   16, iteration   50, batch loss value: 0.47558\n",
      "Epoch   16, iteration   60, batch loss value: 0.46537\n",
      "Epoch   16, iteration   70, batch loss value: 0.46612\n",
      "Epoch   16, iteration   80, batch loss value: 0.44375\n",
      "Epoch   16, iteration   90, batch loss value: 0.46468\n",
      "Epoch   16, Training AUC: 0.84066, Testing AUC: 0.81576\n",
      "Epoch   17, iteration    0, batch loss value: 0.48957\n",
      "Epoch   17, iteration   10, batch loss value: 0.48775\n",
      "Epoch   17, iteration   20, batch loss value: 0.48115\n",
      "Epoch   17, iteration   30, batch loss value: 0.48115\n",
      "Epoch   17, iteration   40, batch loss value: 0.42977\n",
      "Epoch   17, iteration   50, batch loss value: 0.47368\n",
      "Epoch   17, iteration   60, batch loss value: 0.46278\n",
      "Epoch   17, iteration   70, batch loss value: 0.46448\n",
      "Epoch   17, iteration   80, batch loss value: 0.44173\n",
      "Epoch   17, iteration   90, batch loss value: 0.46002\n",
      "Epoch   17, Training AUC: 0.84205, Testing AUC: 0.81645\n",
      "Epoch   18, iteration    0, batch loss value: 0.48941\n",
      "Epoch   18, iteration   10, batch loss value: 0.48321\n",
      "Epoch   18, iteration   20, batch loss value: 0.48593\n",
      "Epoch   18, iteration   30, batch loss value: 0.47808\n",
      "Epoch   18, iteration   40, batch loss value: 0.42868\n",
      "Epoch   18, iteration   50, batch loss value: 0.47057\n",
      "Epoch   18, iteration   60, batch loss value: 0.46166\n",
      "Epoch   18, iteration   70, batch loss value: 0.45821\n",
      "Epoch   18, iteration   80, batch loss value: 0.44122\n",
      "Epoch   18, iteration   90, batch loss value: 0.46178\n",
      "Epoch   18, Training AUC: 0.84353, Testing AUC: 0.81656\n",
      "Epoch   19, iteration    0, batch loss value: 0.50141\n",
      "Epoch   19, iteration   10, batch loss value: 0.48485\n",
      "Epoch   19, iteration   20, batch loss value: 0.47974\n",
      "Epoch   19, iteration   30, batch loss value: 0.48\n",
      "Epoch   19, iteration   40, batch loss value: 0.42585\n",
      "Epoch   19, iteration   50, batch loss value: 0.46898\n",
      "Epoch   19, iteration   60, batch loss value: 0.46196\n",
      "Epoch   19, iteration   70, batch loss value: 0.46662\n",
      "Epoch   19, iteration   80, batch loss value: 0.43998\n",
      "Epoch   19, iteration   90, batch loss value: 0.45795\n",
      "Epoch   19, Training AUC: 0.84404, Testing AUC: 0.81576\n",
      "Epoch   20, iteration    0, batch loss value: 0.48957\n",
      "Epoch   20, iteration   10, batch loss value: 0.47807\n",
      "Epoch   20, iteration   20, batch loss value: 0.4802\n",
      "Epoch   20, iteration   30, batch loss value: 0.48114\n",
      "Epoch   20, iteration   40, batch loss value: 0.42296\n",
      "Epoch   20, iteration   50, batch loss value: 0.46653\n",
      "Epoch   20, iteration   60, batch loss value: 0.45582\n",
      "Epoch   20, iteration   70, batch loss value: 0.46471\n",
      "Epoch   20, iteration   80, batch loss value: 0.43901\n",
      "Epoch   20, iteration   90, batch loss value: 0.45448\n",
      "Epoch   20, Training AUC: 0.84553, Testing AUC: 0.8167\n",
      "Epoch   21, iteration    0, batch loss value: 0.48634\n",
      "Epoch   21, iteration   10, batch loss value: 0.48405\n",
      "Epoch   21, iteration   20, batch loss value: 0.47398\n",
      "Epoch   21, iteration   30, batch loss value: 0.47858\n",
      "Epoch   21, iteration   40, batch loss value: 0.42246\n",
      "Epoch   21, iteration   50, batch loss value: 0.46487\n",
      "Epoch   21, iteration   60, batch loss value: 0.45828\n",
      "Epoch   21, iteration   70, batch loss value: 0.45491\n",
      "Epoch   21, iteration   80, batch loss value: 0.4391\n",
      "Epoch   21, iteration   90, batch loss value: 0.45377\n",
      "Epoch   21, Training AUC: 0.84681, Testing AUC: 0.81646\n",
      "Epoch   22, iteration    0, batch loss value: 0.48532\n",
      "Epoch   22, iteration   10, batch loss value: 0.48097\n",
      "Epoch   22, iteration   20, batch loss value: 0.48641\n",
      "Epoch   22, iteration   30, batch loss value: 0.48709\n",
      "Epoch   22, iteration   40, batch loss value: 0.42073\n",
      "Epoch   22, iteration   50, batch loss value: 0.46299\n",
      "Epoch   22, iteration   60, batch loss value: 0.45274\n",
      "Epoch   22, iteration   70, batch loss value: 0.45534\n",
      "Epoch   22, iteration   80, batch loss value: 0.43444\n",
      "Epoch   22, iteration   90, batch loss value: 0.44937\n",
      "Epoch   22, Training AUC: 0.84832, Testing AUC: 0.81776\n",
      "Epoch   23, iteration    0, batch loss value: 0.47871\n",
      "Epoch   23, iteration   10, batch loss value: 0.47361\n",
      "Epoch   23, iteration   20, batch loss value: 0.47624\n",
      "Epoch   23, iteration   30, batch loss value: 0.47327\n",
      "Epoch   23, iteration   40, batch loss value: 0.41891\n",
      "Epoch   23, iteration   50, batch loss value: 0.46327\n",
      "Epoch   23, iteration   60, batch loss value: 0.45096\n",
      "Epoch   23, iteration   70, batch loss value: 0.45011\n",
      "Epoch   23, iteration   80, batch loss value: 0.43197\n",
      "Epoch   23, iteration   90, batch loss value: 0.45134\n",
      "Epoch   23, Training AUC: 0.84922, Testing AUC: 0.81636\n",
      "Epoch   24, iteration    0, batch loss value: 0.47933\n",
      "Epoch   24, iteration   10, batch loss value: 0.47448\n",
      "Epoch   24, iteration   20, batch loss value: 0.47529\n",
      "Epoch   24, iteration   30, batch loss value: 0.47721\n",
      "Epoch   24, iteration   40, batch loss value: 0.41749\n",
      "Epoch   24, iteration   50, batch loss value: 0.46123\n",
      "Epoch   24, iteration   60, batch loss value: 0.45001\n",
      "Epoch   24, iteration   70, batch loss value: 0.4487\n",
      "Epoch   24, iteration   80, batch loss value: 0.43258\n",
      "Epoch   24, iteration   90, batch loss value: 0.44569\n",
      "Epoch   24, Training AUC: 0.85004, Testing AUC: 0.81645\n",
      "program run for: 3831.325211286545s\n"
     ]
    }
   ],
   "source": [
    "WITH_CONFIG = True\n",
    "num_epochs = 25\n",
    "\n",
    "start_time = time.time()\n",
    "logger.info(\"Start the program...\")\n",
    "if WITH_CONFIG:\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        optimize(sess)\n",
    "else:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        optimize(sess)\n",
    "           \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"program run for: {0}s\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
